#!/usr/bin/python3.8

# =================================================================
#                   Question object creation
# =================================================================
# # Example of potential procedural question creation
#
# for i in range(1,4):
#     for j in range(1,6):
#         qid = f'{i}-{j}'
#         main_compile = CompileTest(points=1, provided_files = [],
#                                    submitted_files=[f'main-{i}-{j}.cpp', f'function-{i}-{j}.cpp'])
#         test_compile = CompileTest(points=1, provided_files = [f'test-{i}-{j}.cpp'],
#                                    submitted_files=[f'function-{i}-{j}.cpp'])
#         questions.append(Question(question_id=qid, max_points=5, file_points=1, test_points=1,
#                                   compile_tests=[main_compile,test_compile], tester_idx=1))
#
# questions.append(Question("FileTest", max_points=1, file_points=1, extra_files=["plan.txt", "solution-1-1.txt"]))

# import os

# if os.path.exists("autograder_util.py") == False:
#     print("Required file autograder_util.py not found. Downloading...")
#     from urllib.request import urlopen

#     url = "https://raw.githubusercontent.com/rhys-brailsford/gs_autograder/main/autograder_util.py"

#     response = urlopen(url)
#     with open("autograder_util.py", "w") as f: f.write(response.read().decode())


#Created by Nilesh Ramgolam(a1814043)

import json
import autograder_util as ag

#Get file points 
# Get compilation points
# Get test points
# sum 
# apply cap 
# apply penalty 
# apply max over points


# ========================================================================
#              Set to true if assignment is a workshop.
# If true, overwrites final score to participation_grade regardless of tests
# for participation grade.
participation_only = False
participation_grade = 1
# ========================================================================

# List of questions objects
questions = []

# 1-1
c_tests = []
c_tests.append(ag.CompileTest(points=1, provided_files=[], submitted_files=["ws9.cpp"]))
q = ag.Question("1-1", max_points=10, file_points=1, test_points=8, compile_tests=c_tests, tester_idx=0)
questions.append(q)

c_tests = []
c_tests.append(ag.CompileTest(points=1, provided_files=[], submitted_files=["ws9.cpp"]))
q = ag.Question("1-2", max_points=10, file_points=1, test_points=8, compile_tests=c_tests, tester_idx=0)
questions.append(q)

c_tests = []
c_tests.append(ag.CompileTest(points=1, provided_files=[], submitted_files=["ws9.cpp"]))
q = ag.Question("1-3", max_points=10, file_points=1, test_points=8, compile_tests=c_tests, tester_idx=0)
questions.append(q)



result_json = ag.run_questions(questions, participation_only, participation_grade)

# close Gradescope results file
with open('results/results.json', 'w') as file:
    json.dump(result_json, file)

final_score = -1
try:
    final_score = result_json["score"]
except:
    print("result_json key error: Missing score key.")
    print(result_json)
print(f'Final grade:{final_score}')
