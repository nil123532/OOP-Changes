#!/usr/bin/python3.8

#create a default result json file
import json

result_json = {
    "score": -1,
    "stdout_visibility": "visible",
    "output": "No autograder run. Submission will be hand marked."
 }

"""
# =================================================================
#                   Question object creation
# =================================================================
# # Example of potential procedural question creation
#
# for i in range(1,4):
#     for j in range(1,6):
#         qid = f'{i}-{j}'
#         main_compile = CompileTest(points=1, provided_files = [],
#                                    submitted_files=[f'main-{i}-{j}.cpp', f'function-{i}-{j}.cpp'])
#         test_compile = CompileTest(points=1, provided_files = [f'test-{i}-{j}.cpp'],
#                                    submitted_files=[f'function-{i}-{j}.cpp'])
#         questions.append(Question(question_id=qid, max_points=5, file_points=1, test_points=1,
#                                   compile_tests=[main_compile,test_compile], tester_idx=1))
#
# questions.append(Question("FileTest", max_points=1, file_points=1, extra_files=["plan.txt", "solution-1-1.txt"]))
"""

import json
import autograder_util as ag


# ========================================================================
#              Set to true if assignment is a workshop.
# If true, overwrites final score to participation_grade regardless of tests
# for participation grade.
participation_only = False
participation_grade = 1
# ========================================================================

# List of questions objects
questions = []

# Class and file names for this specific session
abstract = "Vehicle"
child = "Car"
grandchild = "Truck"
container = "Fleet"


id = "1-1"
c_tests = []
c_tests.append(ag.CompileTest(submitted_files=[f'main-{id}.cpp', f'{abstract}.cpp'],
                           points=5))
c_tests.append(ag.CompileTest(submitted_files=[f'{abstract}.cpp'],
                           provided_files=[f'test-{id}.cpp'], points=2))
q = ag.Question(question_id=id, max_points=15, file_points=1, test_points=7,
             compile_tests=c_tests, tester_idx=1, extra_files=[f'{abstract}.h'])
questions.append(q)


id = "2-1"
c_tests = []
c_tests.append(ag.CompileTest(submitted_files=[f'main-{id}.cpp', f'{child}.cpp', f'{abstract}.cpp'],
                           points=3))
c_tests.append(ag.CompileTest(submitted_files=[f'{child}.cpp', f'{abstract}.cpp'],
                           provided_files=[f'test-{id}.cpp'], points=4))
q = ag.Question(question_id=id, max_points=15, file_points=1, test_points=7,
             compile_tests=c_tests, tester_idx=1, extra_files=[f'{child}.h'])
questions.append(q)

id = "2-2"
c_tests = []
c_tests.append(ag.CompileTest(submitted_files=[f'main-{id}.cpp', f'{grandchild}.cpp', f'{child}.cpp', f'{abstract}.cpp'],
                           points=3))
c_tests.append(ag.CompileTest(submitted_files=[f'{grandchild}.cpp', f'{child}.cpp', f'{abstract}.cpp'],
                           provided_files=[f'test-{id}.cpp'], points=2))
q = ag.Question(question_id=id, max_points=10, file_points=1, test_points=4,
             compile_tests=c_tests, tester_idx=1, extra_files=[f'{grandchild}.h'])
questions.append(q)


id = "3-1"
c_tests = []
c_tests.append(ag.CompileTest(submitted_files=[f'main-{id}.cpp', f'{grandchild}.cpp', f'{container}.cpp', f'{child}.cpp', f'{abstract}.cpp'],
                           points=3))
c_tests.append(ag.CompileTest(submitted_files=[f'{grandchild}.cpp', f'{container}.cpp', f'{child}.cpp', f'{abstract}.cpp'],
                           provided_files=[f'test-{id}.cpp'], points=2))
q = ag.Question(question_id=id, max_points=10, file_points=1, test_points=9,
             compile_tests=c_tests, tester_idx=1, extra_files=[f'{container}.h'])
questions.append(q)

result_json = ag.run_questions(questions, participation_only, participation_grade)


# close Gradescope results file
with open('results/results.json', 'w') as file:
    json.dump(result_json, file)


final_score = -1
try:
    final_score = result_json["score"]
except:
    print("result_json key error: Missing score key.")
    print(result_json)
print(f'Final grade:{final_score}')
